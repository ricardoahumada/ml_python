{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "117b7cb2",
   "metadata": {},
   "source": [
    "*ref: https://inria.github.io/scikit-learn-mooc/python_scripts/ensemble_random_forest.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f497511",
   "metadata": {},
   "source": [
    "Random forests have another particularity: **when training a tree, the search for the best split is done only on a subset of the original features taken at random.**\n",
    "\n",
    "The random subsets are different for each split node. The goal is to inject additional randomization into the learning procedure to try to decorrelate the prediction errors of the individual trees.\n",
    "\n",
    "Therefore, random forests are using randomization on both axes of the data matrix:\n",
    "- by bootstrapping samples for each tree in the forest;\n",
    "- randomly selecting a subset of features at each node of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3a2fa",
   "metadata": {},
   "source": [
    "# A look at random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d529de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will illustrate the usage of a random forest classifier on the adult census dataset.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "adult_census = pd.read_csv(\"../../datasets/adult-census.csv\")\n",
    "target_name = \"class\"\n",
    "data = adult_census.drop(columns=[target_name, \"education-num\"])\n",
    "target = adult_census[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f65ba9",
   "metadata": {},
   "source": [
    ">The adult census contains some categorical data and we encode the categorical features using an OrdinalEncoder since tree-based models can work very efficiently with such a naive representation of categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373c3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "categorical_encoder = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\", unknown_value=-1\n",
    ")\n",
    "preprocessor = make_column_transformer(\n",
    "    (categorical_encoder, make_column_selector(dtype_include=object)),\n",
    "    remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7baeb60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first give a simple example where we will train a single decision tree classifier \n",
    "# and check its generalization performance via cross-validation.\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = make_pipeline(preprocessor, DecisionTreeClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14b7340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree classifier: 0.820 ± 0.006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_tree = cross_val_score(tree, data, target)\n",
    "\n",
    "print(f\"Decision tree classifier: \"\n",
    "      f\"{scores_tree.mean():.3f} ± {scores_tree.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed62f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we construct a BaggingClassifier with a decision tree classifier as base model.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagged_trees = make_pipeline(\n",
    "    preprocessor,\n",
    "    BaggingClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(random_state=0),\n",
    "        n_estimators=50, n_jobs=2, random_state=0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23d8f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged decision tree classifier: 0.846 ± 0.005\n"
     ]
    }
   ],
   "source": [
    "scores_bagged_trees = cross_val_score(bagged_trees, data, target)\n",
    "\n",
    "print(f\"Bagged decision tree classifier: \"\n",
    "      f\"{scores_bagged_trees.mean():.3f} ± {scores_bagged_trees.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b44531c",
   "metadata": {},
   "source": [
    ">Observe that we do not need to specify any base_estimator because the estimator is forced to be a decision tree. Thus, we just specify the desired number of trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef30e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(n_estimators=50, n_jobs=2, random_state=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc5b7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(n_estimators=50, n_jobs=2, random_state=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa64bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier: 0.851 ± 0.004\n"
     ]
    }
   ],
   "source": [
    "scores_random_forest = cross_val_score(random_forest, data, target)\n",
    "\n",
    "print(f\"Random forest classifier: \"\n",
    "      f\"{scores_random_forest.mean():.3f} ± \"\n",
    "      f\"{scores_random_forest.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e61ba",
   "metadata": {},
   "source": [
    "# Details about default hyperparameters\n",
    "For random forests, it is possible to control the amount of randomness for each split by setting the value of max_features hyperparameter:\n",
    "\n",
    "- max_features=0.5 means that 50% of the features are considered at each split;\n",
    "- max_features=1.0 means that all features are considered at each split which effectively disables feature subsampling.\n",
    "\n",
    "By default, RandomForestRegressor disables feature subsampling while RandomForestClassifier uses max_features=np.sqrt(n_features). These default values reflect good practices given in the scientific literature.\n",
    "\n",
    ">However, max_features is one of the hyperparameters to consider when tuning a random forest:\n",
    ">- **too much randomness** in the trees can lead to underfitted base models and can be detrimental for the ensemble as a whole,\n",
    ">- **too few randomness** in the trees leads to more correlation of the prediction errors and as a result reduce the benefits of the averaging step in terms of overfitting control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d91569",
   "metadata": {},
   "source": [
    "**We summarize these details in the following table:**\n",
    "<table class=\"colwidths-auto table\">\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Ensemble model class</p></th>\n",
    "<th class=\"head\"><p>Base model class</p></th>\n",
    "<th class=\"head\"><p>Default value for <code class=\"docutils literal notranslate\"><span class=\"pre\">max_features</span></code></p></th>\n",
    "<th class=\"head\"><p>Features subsampling strategy</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">BaggingClassifier</span></code></p></td>\n",
    "<td><p>User specified (flexible)</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">n_features</span></code> (no&nbsp;subsampling)</p></td>\n",
    "<td><p>Model level</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">RandomForestClassifier</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">DecisionTreeClassifier</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">sqrt(n_features)</span></code></p></td>\n",
    "<td><p>Tree node level</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">BaggingRegressor</span></code></p></td>\n",
    "<td><p>User specified (flexible)</p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">n_features</span></code> (no&nbsp;subsampling)</p></td>\n",
    "<td><p>Model level</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">RandomForestRegressor</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">DecisionTreeRegressor</span></code></p></td>\n",
    "<td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">n_features</span></code> (no&nbsp;subsampling)</p></td>\n",
    "<td><p>Tree node level</p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
