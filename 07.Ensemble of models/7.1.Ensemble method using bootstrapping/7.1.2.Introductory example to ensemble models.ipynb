{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86db39ed",
   "metadata": {},
   "source": [
    "*ref: https://inria.github.io/scikit-learn-mooc/python_scripts/ensemble_introduction.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26af4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "target *= 100  # rescale the target in k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139c4be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score obtained by cross-validation: 0.354 ± 0.087\n"
     ]
    }
   ],
   "source": [
    "# We will check the generalization performance of decision tree regressor with default parameters.\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "cv_results = cross_validate(tree, data, target, n_jobs=2)\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "      f\"{scores.mean():.3f} ± {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767ac26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score obtained by cross-validation: 0.523 ± 0.107\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Now, we make a grid-search to tune the hyperparameters that we mentioned earlier.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [5, 8, None],\n",
    "    \"min_samples_split\": [2, 10, 30, 50],\n",
    "    \"min_samples_leaf\": [0.01, 0.05, 0.1, 1]}\n",
    "cv = 3\n",
    "\n",
    "tree = GridSearchCV(DecisionTreeRegressor(random_state=0),\n",
    "                    param_grid=param_grid, cv=cv, n_jobs=2)\n",
    "cv_results = cross_validate(tree, data, target, n_jobs=2,\n",
    "                            return_estimator=True)\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "      f\"{scores.mean():.3f} ± {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7b2ef",
   "metadata": {},
   "source": [
    "**Now we will use an ensemble method called bagging.**\n",
    "\n",
    "Here, we will use 20 decision trees and check the fitting time as well as the generalization performance on the left-out testing data. It is important to note that we are not going to tune any parameter of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898b505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score obtained by cross-validation: 0.642 ± 0.083\n",
      "CPU times: total: 156 ms\n",
      "Wall time: 7.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "base_estimator = DecisionTreeRegressor(random_state=0)\n",
    "bagging_regressor = BaggingRegressor(\n",
    "    base_estimator=base_estimator, n_estimators=20, random_state=0)\n",
    "\n",
    "cv_results = cross_validate(bagging_regressor, data, target, n_jobs=2)\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "      f\"{scores.mean():.3f} ± {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37dde1",
   "metadata": {},
   "source": [
    ">Without searching for optimal hyperparameters, the overall generalization performance of the bagging regressor is better than a single decision tree.\\\n",
    "In addition, the computational cost is reduced in comparison of seeking for the optimal hyperparameters.\n",
    "\n",
    ">This shows the motivation behind the use of an ensemble learner: it gives a relatively good baseline with decent generalization performance without any parameter tuning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f8e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
